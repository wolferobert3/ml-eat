{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "from os import path, listdir\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from PIL import Image\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from mleat import ML_EAT\n",
    "from utils import read_json_file, get_torchtext_vectors, get_lm_embeddings, get_clip_text_embs, get_clip_image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove ML-EAT measurements\n",
    "# -----------------------\n",
    "# Path constants\n",
    "JSON_PATH = './eat_stimuli/text_stimuli'\n",
    "DF_PATH = './'\n",
    "\n",
    "# ML-EAT constants\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Load embedding\n",
    "embedding = torchtext.vocab.GloVe(name=\"840B\", dim=300)\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for i in range(1, 11):\n",
    "    stimuli = read_json_file(path.join(JSON_PATH, f'weat_{i}.json'))\n",
    "\n",
    "    A = get_torchtext_vectors(embedding, stimuli['A'])\n",
    "    B = get_torchtext_vectors(embedding, stimuli['B'])\n",
    "    X = get_torchtext_vectors(embedding, stimuli['X'])\n",
    "    Y = get_torchtext_vectors(embedding, stimuli['Y'])\n",
    "\n",
    "    ml_eat_results = ML_EAT(\n",
    "        A=A,\n",
    "        B=B,\n",
    "        X=X,\n",
    "        Y=Y,\n",
    "        permutations=PERMUTATIONS,\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(ml_eat_results, index=[stimuli['eat_name']])\n",
    "    result_dfs.append(df)\n",
    "\n",
    "# Concatenate dataframes and write to CSV\n",
    "glove_df = pd.concat(result_dfs)\n",
    "glove_df.to_csv(path.join(DF_PATH, 'glove_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News Vectors ML-EAT measurements\n",
    "\n",
    "# Google News ML-EAT measurements\n",
    "# -----------------------\n",
    "# Path constants\n",
    "JSON_PATH = './eat_stimuli/text_stimuli'\n",
    "DF_PATH = './'\n",
    "\n",
    "# ML-EAT constants\n",
    "PERMUTATIONS = 10000\n",
    "\n",
    "# Google News Vectors link\n",
    "GOOGLE_VECS_LINK = ''\n",
    "\n",
    "# Load embedding\n",
    "embedding = torchtext.vocab.Vectors(name=GOOGLE_VECS_LINK)\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for i in range(1, 11):\n",
    "    stimuli = read_json_file(path.join(JSON_PATH, f'weat_{i}.json'))\n",
    "\n",
    "    # Apply any word2vec replacements\n",
    "    if i == 2:\n",
    "        stimuli['Y'] = [i for i in stimuli['Y'] if i != 'axe']\n",
    "        stimuli['X'] = [i for i in stimuli['X'] if i != 'banjo']\n",
    "    \n",
    "    if i == 5:\n",
    "        stimuli['X'] += ['Jay', 'Kristen']\n",
    "        stimuli['Y'] += ['Tremayne', 'Latonya']\n",
    "    \n",
    "    if i == 9:\n",
    "        stimuli['A'] = [i for i in stimuli['A'] if i not in ('short-term')]\n",
    "        stimuli['A'] += ['short']\n",
    "\n",
    "    # Get word embeddings\n",
    "    A = get_torchtext_vectors(embedding, stimuli['A'])\n",
    "    B = get_torchtext_vectors(embedding, stimuli['B'])\n",
    "    X = get_torchtext_vectors(embedding, stimuli['X'])\n",
    "    Y = get_torchtext_vectors(embedding, stimuli['Y'])\n",
    "\n",
    "    ml_eat_results = ML_EAT(\n",
    "        A=A,\n",
    "        B=B,\n",
    "        X=X,\n",
    "        Y=Y,\n",
    "        permutations=PERMUTATIONS,\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(ml_eat_results, index=[stimuli['eat_name']])\n",
    "    result_dfs.append(df)\n",
    "\n",
    "# Concatenate dataframes and write to CSV\n",
    "glove_df = pd.concat(result_dfs)\n",
    "glove_df.to_csv(path.join(DF_PATH, 'google_news_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 ML-EAT measurements\n",
    "# --------------------------------\n",
    "# Define constants\n",
    "LANGUAGE_MODELS = [\n",
    "    'gpt2',\n",
    "    'gpt2-medium',\n",
    "    'gpt2-large',\n",
    "    'gpt2-xl',\n",
    "]\n",
    "\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Prompt for semantically neutral sentence\n",
    "PROMPT = 'This is'\n",
    "\n",
    "# Define vowels for a/an adjustment in prompts\n",
    "VOWELS = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "# Lambda function to adjust a/an based on first letter of target word\n",
    "vowel_adjust = lambda x: f'{PROMPT} an {x}' if x[0] in VOWELS else f'{PROMPT} a {x}'\n",
    "\n",
    "# Lambda function for target groups composed of words that don't take articles\n",
    "prompt_adjust = lambda x: f'{PROMPT} {x}'\n",
    "\n",
    "# Dictionary to map prompt adjustment functions to tests\n",
    "adjustment_dict = {'0': prompt_adjust, '1': vowel_adjust}\n",
    "\n",
    "# Specify prompt adjustments for each test\n",
    "PROMPT_ADJUSTMENTS = [\n",
    "    '01',\n",
    "    '01',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "    '00',\n",
    "]\n",
    "\n",
    "# Define path constants - weat_1 corresponds to pleasant/unpleasant, flowers/insects\n",
    "WEAT_PATH = './eat_stimuli/text_stimuli/'\n",
    "DF_PATH = './'\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for model_name in LANGUAGE_MODELS:\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        \n",
    "        # Load WEAT stimuli\n",
    "        text_stim = read_json_file(path.join(WEAT_PATH, f'weat_{i+1}.json'))\n",
    "\n",
    "        # Define attribute and target word sets - pleasant, unpleasant, flowers, insects\n",
    "        A, B, X, Y = text_stim['A'], text_stim['B'], text_stim['X'], text_stim['Y']\n",
    "\n",
    "        # Get prompt adjustment functions\n",
    "        attribute_adjust = adjustment_dict[PROMPT_ADJUSTMENTS[i][0]]\n",
    "        target_adjust = adjustment_dict[PROMPT_ADJUSTMENTS[i][1]]\n",
    "\n",
    "        # Create prompts for each word in the sets\n",
    "        A_Prompts = [attribute_adjust(a) for a in A]\n",
    "        B_Prompts = [attribute_adjust(b) for b in B]\n",
    "        X_Prompts = [target_adjust(x) for x in X]\n",
    "        Y_Prompts = [target_adjust(y) for y in Y]\n",
    "\n",
    "        # Get embeddings for each set of words\n",
    "        A_Embs = get_lm_embeddings(model, tokenizer, A_Prompts)\n",
    "        B_Embs = get_lm_embeddings(model, tokenizer, B_Prompts)\n",
    "        X_Embs = get_lm_embeddings(model, tokenizer, X_Prompts)\n",
    "        Y_Embs = get_lm_embeddings(model, tokenizer, Y_Prompts)\n",
    "\n",
    "        # Run ML-EAT\n",
    "        ml_eat_results = ML_EAT(\n",
    "            A=A_Embs,\n",
    "            B=B_Embs,\n",
    "            X=X_Embs,\n",
    "            Y=Y_Embs,\n",
    "            permutations=PERMUTATIONS,\n",
    "        )\n",
    "\n",
    "        # Keep track of model\n",
    "        ml_eat_results['model_test'] = f'{model_name}_{text_stim[\"eat_name\"]}'\n",
    "        ml_eat_results['model'] = model_name\n",
    "        ml_eat_results['test'] = text_stim['eat_name']\n",
    "\n",
    "        # Convert results to dataframe and append to list\n",
    "        df = pd.DataFrame(ml_eat_results, index=[0])\n",
    "        result_dfs.append(df)\n",
    "\n",
    "# Concatenate dataframes and write to CSV\n",
    "lm_df = pd.concat(result_dfs)\n",
    "lm_df.to_csv(path.join(DF_PATH, 'gpt2_results.csv'), index='model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP ML-EAT measurements\n",
    "# ----------------------------\n",
    "# Set models to be tested\n",
    "CLIP_MODELS = [\n",
    "    'openai/clip-vit-base-patch32',\n",
    "    'openai/clip-vit-base-patch16',\n",
    "    'openai/clip-vit-large-patch14',\n",
    "    'openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "\n",
    "# Define constants for ML-EAT\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Define path constants\n",
    "TEXT_PATHS = [\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_3.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_6.json',\n",
    "    './eat_stimuli/text_stimuli/weat_8.json',\n",
    "]\n",
    "\n",
    "IMAGE_PATHS_X = [\n",
    "    './eat_stimuli/image_stimuli/flower-insect/flower/',\n",
    "    './eat_stimuli/image_stimuli/age/young/',\n",
    "    './eat_stimuli/image_stimuli/arab-muslim/other-people/',\n",
    "    './eat_stimuli/image_stimuli/disabled/abled/',\n",
    "    './eat_stimuli/image_stimuli/race/european-american/',\n",
    "    './eat_stimuli/image_stimuli/religion/christianity/',\n",
    "    './eat_stimuli/image_stimuli/sexuality/straight/',\n",
    "    './eat_stimuli/image_stimuli/skin-tone/light/',\n",
    "    './eat_stimuli/image_stimuli/weight/thin/',\n",
    "    './eat_stimuli/image_stimuli/gender/male/',\n",
    "    './eat_stimuli/image_stimuli/gender/science/',\n",
    "]\n",
    "\n",
    "IMAGE_PATHS_Y = [\n",
    "    './eat_stimuli/image_stimuli/flower-insect/insect/',\n",
    "    './eat_stimuli/image_stimuli/age/old/',\n",
    "    './eat_stimuli/image_stimuli/arab-muslim/arab-muslim/',\n",
    "    './eat_stimuli/image_stimuli/disabled/disabled/',\n",
    "    './eat_stimuli/image_stimuli/race/african-american/',\n",
    "    './eat_stimuli/image_stimuli/religion/judaism/',\n",
    "    './eat_stimuli/image_stimuli/sexuality/gay/',\n",
    "    './eat_stimuli/image_stimuli/skin-tone/dark/',\n",
    "    './eat_stimuli/image_stimuli/weight/fat/',\n",
    "    './eat_stimuli/image_stimuli/gender/female/',\n",
    "    './eat_stimuli/image_stimuli/gender/liberal-arts/',\n",
    "]\n",
    "\n",
    "TEST_NAMES = [\n",
    "    'flower-insect',\n",
    "    'age',\n",
    "    'arab-muslim',\n",
    "    'disabled',\n",
    "    'race',\n",
    "    'religion',\n",
    "    'sexuality',\n",
    "    'skin-tone',\n",
    "    'weight',\n",
    "    'gender',\n",
    "    'science-arts',\n",
    "]\n",
    "\n",
    "DF_PATH = './'\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for model_name in CLIP_MODELS:\n",
    "\n",
    "    # Load model, tokenizer, and processor\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    for test_idx in range(len(TEXT_PATHS)):\n",
    "\n",
    "        # Load WEAT stimuli\n",
    "        text_stim = read_json_file(TEXT_PATHS[test_idx])\n",
    "\n",
    "        # Define attribute and target word sets - pleasant, unpleasant\n",
    "        A, B = text_stim['A'], text_stim['B']\n",
    "\n",
    "        # Get image stimuli\n",
    "        IMAGE_STIM_PATH_X = IMAGE_PATHS_X[test_idx]\n",
    "        IMAGE_STIM_PATH_Y = IMAGE_PATHS_Y[test_idx]\n",
    "\n",
    "        imgs_x = listdir(IMAGE_STIM_PATH_X)\n",
    "        imgs_y = listdir(IMAGE_STIM_PATH_Y)\n",
    "\n",
    "        # Open images and convert to PIL Image objects\n",
    "        X = [Image.open(path.join(IMAGE_STIM_PATH_X, img)) for img in imgs_x]\n",
    "        Y = [Image.open(path.join(IMAGE_STIM_PATH_Y, img)) for img in imgs_y]\n",
    "\n",
    "        # Get embeddings for each set of stimuli\n",
    "        A_Embs = get_clip_text_embs(model, tokenizer, A)\n",
    "        B_Embs = get_clip_text_embs(model, tokenizer, B)\n",
    "        X_Embs = get_clip_image_embs(model, processor, X)\n",
    "        Y_Embs = get_clip_image_embs(model, processor, Y)\n",
    "\n",
    "        # Run ML-EAT\n",
    "        ml_eat_results = ML_EAT(\n",
    "            A=A_Embs,\n",
    "            B=B_Embs,\n",
    "            X=X_Embs,\n",
    "            Y=Y_Embs,\n",
    "            permutations=PERMUTATIONS,\n",
    "        )\n",
    "\n",
    "        # Add model name and eat name to results\n",
    "        ml_eat_results['model_test'] = f'{model_name}_{TEST_NAMES[test_idx]}'\n",
    "        ml_eat_results['model'] = model_name\n",
    "        ml_eat_results['test'] = TEST_NAMES[test_idx]\n",
    "\n",
    "        # Convert results to dataframe and append to list\n",
    "        df = pd.DataFrame(ml_eat_results, index=[0])\n",
    "        result_dfs.append(df)\n",
    "\n",
    "# Concatenate dataframes and write to CSV\n",
    "clip_df = pd.concat(result_dfs)\n",
    "clip_df.to_csv(path.join(DF_PATH, 'clip_results.csv'), index='model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP ML-EAT measurements\n",
    "# ----------------------------\n",
    "# Set models to be tested\n",
    "CLIP_MODELS = [\n",
    "    'openai/clip-vit-base-patch32',\n",
    "    'openai/clip-vit-base-patch16',\n",
    "    'openai/clip-vit-large-patch14',\n",
    "    'openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "\n",
    "# Define constants for ML-EAT\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Define path constants\n",
    "TEXT_PATHS = [\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_3.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_1.json',\n",
    "    './eat_stimuli/text_stimuli/weat_6.json',\n",
    "    './eat_stimuli/text_stimuli/weat_8.json',\n",
    "]\n",
    "\n",
    "IMAGE_PATHS_X = [\n",
    "    './eat_stimuli/image_stimuli/flower-insect/flower/',\n",
    "    './eat_stimuli/image_stimuli/age/young/',\n",
    "    './eat_stimuli/image_stimuli/arab-muslim/other-people/',\n",
    "    './eat_stimuli/image_stimuli/disabled/abled/',\n",
    "    './eat_stimuli/image_stimuli/race/european-american/',\n",
    "    './eat_stimuli/image_stimuli/religion/christianity/',\n",
    "    './eat_stimuli/image_stimuli/sexuality/straight/',\n",
    "    './eat_stimuli/image_stimuli/skin-tone/light/',\n",
    "    './eat_stimuli/image_stimuli/weight/thin/',\n",
    "    './eat_stimuli/image_stimuli/gender/male/',\n",
    "    './eat_stimuli/image_stimuli/gender/science/',\n",
    "]\n",
    "\n",
    "IMAGE_PATHS_Y = [\n",
    "    './eat_stimuli/image_stimuli/flower-insect/insect/',\n",
    "    './eat_stimuli/image_stimuli/age/old/',\n",
    "    './eat_stimuli/image_stimuli/arab-muslim/arab-muslim/',\n",
    "    './eat_stimuli/image_stimuli/disabled/disabled/',\n",
    "    './eat_stimuli/image_stimuli/race/african-american/',\n",
    "    './eat_stimuli/image_stimuli/religion/judaism/',\n",
    "    './eat_stimuli/image_stimuli/sexuality/gay/',\n",
    "    './eat_stimuli/image_stimuli/skin-tone/dark/',\n",
    "    './eat_stimuli/image_stimuli/weight/fat/',\n",
    "    './eat_stimuli/image_stimuli/gender/female/',\n",
    "    './eat_stimuli/image_stimuli/gender/liberal-arts/',\n",
    "]\n",
    "\n",
    "TEST_NAMES = [\n",
    "    'flower-insect',\n",
    "    'age',\n",
    "    'arab-muslim',\n",
    "    'disabled',\n",
    "    'race',\n",
    "    'religion',\n",
    "    'sexuality',\n",
    "    'skin-tone',\n",
    "    'weight',\n",
    "    'gender',\n",
    "    'science-arts',\n",
    "]\n",
    "\n",
    "DF_PATH = './'\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for model_name in CLIP_MODELS:\n",
    "\n",
    "    # Load model, tokenizer, and processor\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    for test_idx in range(len(TEXT_PATHS)):\n",
    "\n",
    "        # Load WEAT stimuli\n",
    "        text_stim = read_json_file(TEXT_PATHS[test_idx])\n",
    "\n",
    "        # Define attribute and target word sets - pleasant, unpleasant\n",
    "        A, B = text_stim['A'], text_stim['B']\n",
    "\n",
    "        A = [f'a picture that brings to mind {a}' for a in A]\n",
    "        B = [f'a picture that brings to mind {b}' for b in B]\n",
    "\n",
    "        # Get image stimuli\n",
    "        IMAGE_STIM_PATH_X = IMAGE_PATHS_X[test_idx]\n",
    "        IMAGE_STIM_PATH_Y = IMAGE_PATHS_Y[test_idx]\n",
    "\n",
    "        imgs_x = listdir(IMAGE_STIM_PATH_X)\n",
    "        imgs_y = listdir(IMAGE_STIM_PATH_Y)\n",
    "\n",
    "        # Open images and convert to PIL Image objects\n",
    "        X = [Image.open(path.join(IMAGE_STIM_PATH_X, img)) for img in imgs_x]\n",
    "        Y = [Image.open(path.join(IMAGE_STIM_PATH_Y, img)) for img in imgs_y]\n",
    "\n",
    "        # Get embeddings for each set of stimuli\n",
    "        A_Embs = get_clip_text_embs(model, tokenizer, A)\n",
    "        B_Embs = get_clip_text_embs(model, tokenizer, B)\n",
    "        X_Embs = get_clip_image_embs(model, processor, X)\n",
    "        Y_Embs = get_clip_image_embs(model, processor, Y)\n",
    "\n",
    "        # Run ML-EAT\n",
    "        ml_eat_results = ML_EAT(\n",
    "            A=A_Embs,\n",
    "            B=B_Embs,\n",
    "            X=X_Embs,\n",
    "            Y=Y_Embs,\n",
    "            permutations=PERMUTATIONS,\n",
    "        )\n",
    "\n",
    "        print(f'{model_name}_{TEST_NAMES[test_idx]}')\n",
    "        for key, value in ml_eat_results.items():\n",
    "            print(f'{key}: {value}')\n",
    "\n",
    "        # Add model name and eat name to results\n",
    "        ml_eat_results['model_test'] = f'{model_name}_{TEST_NAMES[test_idx]}'\n",
    "        ml_eat_results['model'] = model_name\n",
    "        ml_eat_results['test'] = TEST_NAMES[test_idx]\n",
    "\n",
    "        # Convert results to dataframe and append to list\n",
    "        df = pd.DataFrame(ml_eat_results, index=[0])\n",
    "        result_dfs.append(df)\n",
    "\n",
    "# Concatenate dataframes and write to CSV\n",
    "clip_df = pd.concat(result_dfs)\n",
    "clip_df.to_csv(path.join(DF_PATH, 'clip_results_prompts.csv'), index='model_test')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
