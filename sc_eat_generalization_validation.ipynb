{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchtext\n",
    "\n",
    "from os import path\n",
    "from utils import read_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Simple class for validating L2 of the ML-EAT\n",
    "class attribute_word:\n",
    "    def __init__(self, word, vector):\n",
    "        self.word = word\n",
    "        self.vector = vector\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def set_T_associations(self, T_vec_list):\n",
    "        self.T_associations = [cosine_similarity(self.vector, t) for t in T_vec_list]\n",
    "    \n",
    "    def set_T_mean(self):\n",
    "        self.T_mean = np.mean(self.T_associations)\n",
    "    \n",
    "    def get_T_mean(self):\n",
    "        return self.T_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple class-based validation of L2 measurements using Glove embeddings\n",
    "# -----------------------\n",
    "# Path constants\n",
    "JSON_PATH = './eat_stimuli/text_stimuli'\n",
    "DF_PATH = './'\n",
    "\n",
    "# ML-EAT constants\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Load embedding\n",
    "embedding = torchtext.vocab.GloVe(name=\"840B\", dim=300)\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for i in range(1, 11):\n",
    "    stimuli = read_json_file(path.join(JSON_PATH, f'weat_{i}.json'))\n",
    "\n",
    "    A_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['A']]\n",
    "    B_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['B']]\n",
    "    X_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['X']]\n",
    "    Y_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['Y']]\n",
    "\n",
    "    A_dict_X = {word: attribute_word(word, vector) for word, vector in zip(stimuli['A'], A_vec_list)}\n",
    "    B_dict_X = {word: attribute_word(word, vector) for word, vector in zip(stimuli['B'], B_vec_list)}\n",
    "    A_dict_Y = {word: attribute_word(word, vector) for word, vector in zip(stimuli['A'], A_vec_list)}\n",
    "    B_dict_Y = {word: attribute_word(word, vector) for word, vector in zip(stimuli['B'], B_vec_list)}\n",
    "\n",
    "    for word in stimuli['A']:\n",
    "        A_dict_X[word].set_T_associations(X_vec_list)\n",
    "        A_dict_X[word].set_T_mean()\n",
    "        A_dict_Y[word].set_T_associations(Y_vec_list)\n",
    "        A_dict_Y[word].set_T_mean()\n",
    "    \n",
    "    for word in stimuli['B']:\n",
    "        B_dict_X[word].set_T_associations(X_vec_list)\n",
    "        B_dict_X[word].set_T_mean()\n",
    "        B_dict_Y[word].set_T_associations(Y_vec_list)\n",
    "        B_dict_Y[word].set_T_mean()\n",
    "\n",
    "    X_effect = (np.mean([A_dict_X[word].get_T_mean() for word in stimuli['A']]) - np.mean([B_dict_X[word].get_T_mean() for word in stimuli['B']])) / np.std(np.concatenate([np.array([A_dict_X[word].get_T_mean() for word in stimuli['A']]), np.array([B_dict_X[word].get_T_mean() for word in stimuli['B']])]), ddof=1)\n",
    "    Y_effect = (np.mean([A_dict_Y[word].get_T_mean() for word in stimuli['A']]) - np.mean([B_dict_Y[word].get_T_mean() for word in stimuli['B']])) / np.std(np.concatenate([np.array([A_dict_Y[word].get_T_mean() for word in stimuli['A']]), np.array([B_dict_Y[word].get_T_mean() for word in stimuli['B']])]), ddof=1)\n",
    "\n",
    "    print(f'WEAT {i} X effect: {X_effect}')\n",
    "    print(f'WEAT {i} Y effect: {Y_effect}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Proof that L2 generalizes the SC-WEAT effect size\n",
    "# -----------------------\n",
    "# Path constants\n",
    "JSON_PATH = './eat_stimuli/text_stimuli'\n",
    "DF_PATH = './'\n",
    "\n",
    "# ML-EAT constants\n",
    "PERMUTATIONS = 10_000\n",
    "\n",
    "# Load embedding\n",
    "embedding = torchtext.vocab.GloVe(name=\"840B\", dim=300)\n",
    "\n",
    "# Store results in a list of dataframes\n",
    "result_dfs = []\n",
    "\n",
    "# Run ML-EAT on each set of WEAT stimuli\n",
    "for i in range(1, 11):\n",
    "    stimuli = read_json_file(path.join(JSON_PATH, f'weat_{i}.json'))\n",
    "\n",
    "    A_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['A']]\n",
    "    B_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['B']]\n",
    "    X_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['X']][:1] # Only one word in X\n",
    "    Y_vec_list = [embedding.get_vecs_by_tokens([word]).numpy().squeeze() for word in stimuli['Y']][:1] # Only one word in Y\n",
    "\n",
    "    # SC-WEAT effect size\n",
    "    x = X_vec_list[0]\n",
    "    X_A = [cosine_similarity(x, a) for a in A_vec_list]\n",
    "    X_B = [cosine_similarity(x, b) for b in B_vec_list]\n",
    "    eff = (np.mean(X_A) - np.mean(X_B)) / np.std(np.concatenate([X_A, X_B]), ddof=1)\n",
    "    print(f'SC-WEAT {i} X effect size: {eff}')\n",
    "\n",
    "    y = Y_vec_list[0]\n",
    "    Y_A = [cosine_similarity(y, a) for a in A_vec_list]\n",
    "    Y_B = [cosine_similarity(y, b) for b in B_vec_list]\n",
    "    eff = (np.mean(Y_A) - np.mean(Y_B)) / np.std(np.concatenate([Y_A, Y_B]), ddof=1)\n",
    "    print(f'SC-WEAT {i} Y effect size: {eff}')\n",
    "\n",
    "    # ML-EAT L2 effect sizes - same code as above\n",
    "    A_dict_X = {word: attribute_word(word, vector) for word, vector in zip(stimuli['A'], A_vec_list)}\n",
    "    B_dict_X = {word: attribute_word(word, vector) for word, vector in zip(stimuli['B'], B_vec_list)}\n",
    "    A_dict_Y = {word: attribute_word(word, vector) for word, vector in zip(stimuli['A'], A_vec_list)}\n",
    "    B_dict_Y = {word: attribute_word(word, vector) for word, vector in zip(stimuli['B'], B_vec_list)}\n",
    "\n",
    "    for word in stimuli['A']:\n",
    "        A_dict_X[word].set_T_associations(X_vec_list)\n",
    "        A_dict_X[word].set_T_mean()\n",
    "        A_dict_Y[word].set_T_associations(Y_vec_list)\n",
    "        A_dict_Y[word].set_T_mean()\n",
    "    \n",
    "    for word in stimuli['B']:\n",
    "        B_dict_X[word].set_T_associations(X_vec_list)\n",
    "        B_dict_X[word].set_T_mean()\n",
    "        B_dict_Y[word].set_T_associations(Y_vec_list)\n",
    "        B_dict_Y[word].set_T_mean()\n",
    "\n",
    "    X_effect = (np.mean([A_dict_X[word].get_T_mean() for word in stimuli['A']]) - np.mean([B_dict_X[word].get_T_mean() for word in stimuli['B']])) / np.std(np.concatenate([np.array([A_dict_X[word].get_T_mean() for word in stimuli['A']]), np.array([B_dict_X[word].get_T_mean() for word in stimuli['B']])]), ddof=1)\n",
    "    Y_effect = (np.mean([A_dict_Y[word].get_T_mean() for word in stimuli['A']]) - np.mean([B_dict_Y[word].get_T_mean() for word in stimuli['B']])) / np.std(np.concatenate([np.array([A_dict_Y[word].get_T_mean() for word in stimuli['A']]), np.array([B_dict_Y[word].get_T_mean() for word in stimuli['B']])]), ddof=1)\n",
    "\n",
    "    print(f'ML-EAT L2 {i} X effect: {X_effect}')\n",
    "    print(f'ML-EAT L2 {i} Y effect: {Y_effect}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
