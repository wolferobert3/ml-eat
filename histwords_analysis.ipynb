{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import path\n",
    "from utils import read_json_file\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def compute_association(w: np.ndarray, A: np.ndarray) -> float:\n",
    "    return np.mean([cosine_similarity(w, a) for a in A])\n",
    "\n",
    "def compute_attribute_association(X: np.ndarray, A: np.ndarray) -> np.ndarray:\n",
    "    return np.array([compute_association(x, A) for x in X])\n",
    "\n",
    "def compute_attribute_association_L2(A: np.ndarray, T: np.ndarray) -> np.ndarray:\n",
    "    return np.array([compute_association(a, T) for a in A])\n",
    "\n",
    "def compute_joint_std(X_Associations: np.ndarray, Y_Associations: np.ndarray) -> float:\n",
    "    return np.std(np.concatenate([X_Associations, Y_Associations]), ddof=1)\n",
    "\n",
    "def compute_p_value(X_Diff: np.ndarray, Y_Diff: np.ndarray, permutations: int=1000) -> float:\n",
    "    test_statistic = np.sum(X_Diff) - np.sum(Y_Diff)\n",
    "    empirical_distribution = np.array([np.random.choice(np.concatenate([X_Diff, Y_Diff]), size=len(X_Diff) + len(Y_Diff), replace=False) for _ in range(permutations)])\n",
    "    empirical_differences = np.sum(empirical_distribution[:, :len(X_Diff)], axis=1) - np.sum(empirical_distribution[:, len(X_Diff):], axis=1)\n",
    "    \n",
    "    return 1-norm.cdf(test_statistic, loc=np.mean(empirical_differences), scale=np.std(empirical_differences, ddof=1))\n",
    "\n",
    "def level_1(X: np.ndarray, Y: np.ndarray, A: np.ndarray, B: np.ndarray, permutations: int=1000) -> float:\n",
    "    X_Associations_A = compute_attribute_association(X, A)\n",
    "    X_Associations_B = compute_attribute_association(X, B)\n",
    "    X_Differential_Associations = X_Associations_A - X_Associations_B\n",
    "\n",
    "    Y_Associations_A = compute_attribute_association(Y, A)\n",
    "    Y_Associations_B = compute_attribute_association(Y, B)\n",
    "    Y_Differential_Associations = Y_Associations_A - Y_Associations_B\n",
    "\n",
    "    X_Mean = np.mean(X_Differential_Associations)\n",
    "    Y_Mean = np.mean(Y_Differential_Associations)\n",
    "\n",
    "    p_value = compute_p_value(X_Differential_Associations, Y_Differential_Associations, permutations=permutations)\n",
    "\n",
    "    return (X_Mean - Y_Mean) / compute_joint_std(X_Differential_Associations, Y_Differential_Associations), p_value\n",
    "\n",
    "def level_2(T: np.ndarray, A: np.ndarray, B: np.ndarray, permutations: int=1000) -> float:\n",
    "    A_Associations_T = compute_attribute_association_L2(A, T)\n",
    "    B_Associations_T = compute_attribute_association_L2(B, T)\n",
    "\n",
    "    p_value = compute_p_value(A_Associations_T, B_Associations_T, permutations=permutations)\n",
    "\n",
    "    return (np.mean(A_Associations_T) - np.mean(B_Associations_T)) / compute_joint_std(A_Associations_T, B_Associations_T), p_value\n",
    "\n",
    "def level_3(T: np.ndarray, A: np.ndarray) -> float:\n",
    "    T_Associations_A = [cosine_similarity(t, a) for t in T for a in A]\n",
    "    return np.mean(T_Associations_A), np.std(T_Associations_A, ddof=1)\n",
    "\n",
    "def ML_EAT(A: np.ndarray, B: np.ndarray, X: np.ndarray, Y: np.ndarray, permutations: int=1000) -> dict:\n",
    "    L1_effect_size, L1_p_value = level_1(X, Y, A, B, permutations=permutations)\n",
    "    L2_effect_size_X, L2_p_value_X = level_2(X, A, B, permutations=permutations)\n",
    "    L2_effect_size_Y, L2_p_value_Y = level_2(Y, A, B, permutations=permutations)\n",
    "    L3_mean_AX, L3_std_AX = level_3(X, A)\n",
    "    L3_mean_BX, L3_std_BX = level_3(X, B)\n",
    "    L3_mean_AY, L3_std_AY = level_3(Y, A)\n",
    "    L3_mean_BY, L3_std_BY = level_3(Y, B)\n",
    "\n",
    "    return {\n",
    "        'L1_effect_size': L1_effect_size,\n",
    "        'L1_p_value': L1_p_value,\n",
    "        'L2_effect_size_X': L2_effect_size_X,\n",
    "        'L2_p_value_X': L2_p_value_X,\n",
    "        'L2_effect_size_Y': L2_effect_size_Y,\n",
    "        'L2_p_value_Y': L2_p_value_Y,\n",
    "        'L3_mean_AX': L3_mean_AX,\n",
    "        'L3_std_AX': L3_std_AX,\n",
    "        'L3_mean_BX': L3_mean_BX,\n",
    "        'L3_std_BX': L3_std_BX,\n",
    "        'L3_mean_AY': L3_mean_AY,\n",
    "        'L3_std_AY': L3_std_AY,\n",
    "        'L3_mean_BY': L3_mean_BY,\n",
    "        'L3_std_BY': L3_std_BY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_np_embeddings(target_words: list,\n",
    "                      vocab_dict: dict,\n",
    "                      embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the embeddings for the target words.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([embeddings[vocab_dict[word]] for word in target_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = './eat_stimuli/text_stimuli'\n",
    "stimuli = read_json_file(path.join(JSON_PATH, f'weat_7.json'))\n",
    "\n",
    "# Replace zero-norm stimuli in the early 1800s embeddings\n",
    "stimuli['Y'][-3] = 'music' # formerly symphony\n",
    "stimuli['X'][0] = 'mathematics' # formerly math\n",
    "stimuli['X'][3] = 'calculation' # formerly calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical embeddings from https://nlp.stanford.edu/projects/histwords/\n",
    "\n",
    "years = list(range(1810, 1991, 10))\n",
    "level1, level2x, level2y = [], [], []\n",
    "l1p, l2xp, l2yp = [], [], []\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    print(f'Year: {year}')\n",
    "    embeddings = np.load(f'./sgns/{year}-w.npy', allow_pickle=True)\n",
    "    vocab = pkl.load(open(f'./sgns/{year}-vocab.pkl', 'rb'))\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    A = get_np_embeddings(stimuli['A'], vocab_dict, embeddings)\n",
    "    B = get_np_embeddings(stimuli['B'], vocab_dict, embeddings)\n",
    "    X = get_np_embeddings(stimuli['X'], vocab_dict, embeddings)\n",
    "    Y = get_np_embeddings(stimuli['Y'], vocab_dict, embeddings)\n",
    "\n",
    "    results = ML_EAT(A, B, X, Y)\n",
    "    \n",
    "    level1.append(results['L1_effect_size'])\n",
    "    level2x.append(results['L2_effect_size_X'])\n",
    "    level2y.append(results['L2_effect_size_Y'])\n",
    "\n",
    "    l1p.append(results['L1_p_value'])\n",
    "    l2xp.append(results['L2_p_value_X'])\n",
    "    l2yp.append(results['L2_p_value_Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(years, level1, label='Level 1', marker='o')\n",
    "plt.plot(years, level2x, label='Level 2 X', marker='o')\n",
    "plt.plot(years, level2y, label='Level 2 Y', marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Effect Size')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX plots\n",
    "print(' '.join([f'({i}, {x})' for i, x in enumerate(level1)]))\n",
    "print(' '.join([f'({i}, {x})' for i, x in enumerate(level2x)]))\n",
    "print(' '.join([f'({i}, {x})' for i, x in enumerate(level2y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe statistical significance to determine EAT patterns\n",
    "print(l1p)\n",
    "print(l2xp)\n",
    "print(l2yp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
